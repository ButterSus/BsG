@file:Suppress("Unused", "MemberVisibilityCanBePrivate")

package com.buttersus.gramutils

import mu.KotlinLogging

/**
 * Base class for all lexers.
 *
 * @param S Self type.
 * @param TT Token type enum.
 * @param TB Token type.
 * @property ğš‚ Source to lex.
 * @property ğš™ Current position.
 */
abstract class LexerBase<S : LexerBase<S, TT, TB>, TT : TypeBase, TB : TokenBase<TT>>(
    protected val details: TypeDetails<TT>,
) : Iterable<TB> {
    // Public properties
    lateinit var `ğš‚`: Source
    lateinit var `ğš™`: Position

    // Logging
    protected val logger = KotlinLogging.logger {}

    // Methods
    private val indentStack = mutableListOf<Int>()

    /**
     * Main method of the lexer, which called every time the parser needs a new token.
     * It must be an iterator, which yields the tokens.
     *
     * @return Iterator of tokens.
     * @see iterator
     */
    protected abstract fun lex(): Iterator<TB>

    private var isFirstToken = true

    /**
     * Tokenizes the source, and returns an iterator of tokens.
     * Note, that during the tokenization, you can interact with the lexer
     * to implement context-sensitive lexing.
     */
    override fun iterator(): Iterator<TB> = iterator {
        logger.info { "Starting..." }
        while (`ğš™`.isNotAtEnd()) {
            Regex("""[^\S\r\n]*""").matchAt(`ğš™`)!!
                .also { this@LexerBase.`ğš™` += it.value.length }
            if (details.generateNewlines && Regex("""\r?\n(?:[^\S\r\n]*\r?\n)*""").matchAt(`ğš™`)?.also {
                    if (details.keepFirstNewline || !isFirstToken) yield(newToken(details.getNewline()!!, it.value))
                    else this@LexerBase.`ğš™` += it.value.length
                } != null
            ) {
                Regex("""[^\S\r\n]*""").matchAt(`ğš™`)!!
                    .also {
                        if (!details.generateIndents) return@also
                        val newIndentLevel = it.value.length
                        val currentIndentLevel = indentStack.lastOrNull() ?: 0
                        when {
                            newIndentLevel > currentIndentLevel -> {
                                indentStack.add(newIndentLevel); yield(
                                    newToken(details.getIndent()!!, it.value)
                                )
                            }

                            newIndentLevel < currentIndentLevel -> {
                                while (newIndentLevel < (indentStack.lastOrNull() ?: 0)) {
                                    indentStack.removeLast()
                                    yield(newToken(details.getDedent()!!, ""))
                                }
                            }
                        }
                    }
                continue
            }
            val iterator = lex()
            val `ğš™â‚€` = `ğš™`.copy()
            if (iterator.hasNext() || `ğš™` != `ğš™â‚€`) yieldAll(iterator)
            else throw Exception("Unexpected character at $`ğš™` -> ${`ğš™`.`ğšŠ`}")
        }
        if (details.generateIndents) indentStack
            .run { forEach { _ -> yield(newToken(details.getDedent()!!, "")) }; clear() }
        if (details.generateNewlines && details.generateLastNewline) `ğš‚`.`ğœ”`.indices.reversed()
            .find { `ğš‚`.`ğœ”`[it] !in "\t\r " }
            ?.let { if (`ğš‚`.`ğœ”`[it] != '\n') yield(newToken(details.getNewline()!!, "")) }
        if (details.generateEOF) yield(newToken(details.getEOF()!!, ""))
        logger.info { "Finished" }
    }

    /**
     * Yields a token if the given pattern matches the source at the current position.
     * To recognize if the pattern matches, check if the returned value **is** `null`.
     *
     * Usage:
     * ```
     * yieldRegex("""\d+""", TokenType.NUMBER) ?: return
     * ```
     */
    protected suspend fun SequenceScope<TB>.yieldRegex(pattern: String, type: TT, vararg flags: RegexOption): Unit? =
        Regex(pattern, flags.toSet() + RegexOption.DOT_MATCHES_ALL).matchAt(`ğš™`)
            ?.also { yield(newToken(type, it.value)) }
            .let { if (it == null) Unit else null }

    protected fun SequenceScope<TB>.skipRegex(pattern: String, vararg flags: RegexOption): Unit? =
        Regex(pattern, flags.toSet() + RegexOption.DOT_MATCHES_ALL).matchAt(`ğš™`)
            ?.also { `ğš™` += it.value.length; logger.trace { "Skipped: ${it.value}" } }
            .let { if (it == null) Unit else null }

    /**
     * Creates a new token of the given type and value.
     * It's used by the lexer to create tokens, so you must override it.
     */
    protected abstract fun createToken(`ğšƒ`: TT, `ğšŸ`: String): TB

    /**
     * Creates a new token of the given type and value,
     * also updates the current position.
     *
     * @param ğšƒ Type of the token.
     * @param ğšŸ Value of the token.
     * @return The created token.
     */
    protected fun newToken(`ğšƒ`: TT, `ğšŸ`: String): TB = createToken(`ğšƒ`, `ğšŸ`)
        .also { token ->
            if (!details.keepFirstNewline) isFirstToken = false
            `ğš™` += `ğšŸ`.length
            logger.trace { token.toFormattedString() }
        }

    /**
     * Resets the lexer to its initial state,
     * and sets the source to the given one.
     * _(Useful for reusing the same lexer)_
     *
     * @param ğš‚ Source to lex.
     * @return This lexer.
     */
    operator fun invoke(`ğš‚`: Source): S {
        this.ğš‚ = ğš‚
        this.`ğš™` = Position(ğš‚)
        @Suppress("UNCHECKED_CAST")
        return this as S
    }
}